# -*- coding: utf-8 -*-
"""temperature_regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ykKCLusaWrb7_Njw9gEtGZnofmuSLl9r
"""

from airflow import DAG
from airflow.operators.python import PythonOperator, BranchPythonOperator
from airflow.operators.empty import EmptyOperator

import pendulum
import requests
import pandas as pd

from sklearn.linear_model import LinearRegression
from sklearn.impute import SimpleImputer
import numpy as np

import boto3
import io

import time
from datetime import datetime, timedelta
import pytz
central = pytz.timezone("US/Central")

S3_BUCKET = "kalenka-mwaa"
INPUT_PREFIX = "weather_data/"
OUTPUT_PREFIX = "predictions/"

WORKFLOW_SCHEDULE = timedelta(hours=20)
WEATHER_STATIONS = ["KORD", "KENW", "KMDW", "KPNT"]

# Task: Linear Regression for Temp
def temperature_regression(**kwargs):
    s3 = boto3.client("s3")

    # List and download all weather files
    response = s3.list_objects_v2(Bucket=S3_BUCKET, Prefix=INPUT_PREFIX)
    objects = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith(".csv")]

    all_dfs = []
    for key in objects:
        obj = s3.get_object(Bucket=S3_BUCKET, Key=key)
        df = pd.read_csv(io.BytesIO(obj['Body'].read()))
        all_dfs.append(df)

    if len(all_dfs) < 10:
        print("Not enough data yet (need 10 CSVs for 20 hours). Skipping.")
        return

    full_df = pd.concat(all_dfs)
    # Remove any features with all null values
    full_df = full_df.dropna(how='all')
    # remove any measurments with null temp
    full_df = full_df.dropna(subset=["temperature"])

    full_df["timestamp"] = pd.to_datetime(full_df["timestamp"])
    full_df["hour"] = full_df["timestamp"].dt.hour
    full_df["minute"] = full_df["timestamp"].dt.minute
    full_df["station_cat"] = full_df["station"].astype("category").cat.codes

    feature_cols = ["dewpoint", "windSpeed", "barometricPressure", "visibility",
                    "precipitationLastHour", "relativeHumidity", "heatIndex", "hour", "minute", "station_cat"]

    # Median imputation
    # Typically wind or heat index values are missing
    X_raw = full_df[feature_cols]
    X_raw = X_raw.dropna(axis=1, how='all')
    feature_cols = X_raw.columns.tolist()
    imputer = SimpleImputer(strategy="median")
    X = imputer.fit_transform(X_raw)

    y = full_df["temperature"]

    # Train model
    model = LinearRegression()
    model.fit(X, y)


    full_df = full_df.dropna(subset=feature_cols)
    X = full_df[feature_cols]
    y = full_df["temperature"]

    # Train model
    model = LinearRegression()
    model.fit(X, y)


    now = datetime.now(central).replace(second=0, microsecond=0)
    # Times to predict at
    steps = [now + timedelta(minutes=30 * i) for i in range(1, 17)]

    future_rows = []
    # Make prediction at each time at each station
    for station in full_df["station"].unique():
        station_cat = full_df[full_df["station"] == station]["station_cat"].iloc[0]
        latest = full_df[full_df["station"] == station].sort_values("timestamp").iloc[-1]

        for step in steps:
            row = {
                "hour": step.hour,
                "minute": step.minute,
                "station_cat": station_cat,
                "dewpoint": latest["dewpoint"],
                "windSpeed": latest["windSpeed"],
                "barometricPressure": latest["barometricPressure"],
                "visibility": latest["visibility"],
                "precipitationLastHour": latest["precipitationLastHour"],
                "relativeHumidity": latest["relativeHumidity"],
                "heatIndex": latest["heatIndex"],
                "station": station,
                "predict_time": step.isoformat()
            }
            future_rows.append(row)

    pred_df = pd.DataFrame(future_rows)
    pred_X = pred_df[feature_cols]
    pred_df["predicted_temperature"] = model.predict(pred_X)

    # Upload predictions
    ts = now.strftime('%Y%m%dT%H%M%S')
    key = f"{OUTPUT_PREFIX}weather_preds_{ts}.csv"

    csv_buffer = io.StringIO()
    pred_df.to_csv(csv_buffer, index=False)
    s3.put_object(Bucket=S3_BUCKET, Key=key, Body=csv_buffer.getvalue())
    print(f"Uploaded prediction to s3://{S3_BUCKET}/{key}")

# DAG Setup
default_args = {
    'owner': 'kalenka',
    'start_date': datetime(2025, 6, 5),
    # These retry parameters will ensure that it will run with 10 csvs (20 hours of data) as the interval is 2h and this is a 2.5hr retry period with 30min intervals
    'retries': 5,
    'retry_delay': timedelta(minutes=30)
}

dag = DAG(
    'train_weather_model',
    default_args=default_args,
    description='Train linear regression to predict temperature after 20hr/40hr for the next 8hr in 30min intervals',
    schedule_interval=WORKFLOW_SCHEDULE,
    catchup=False,
    tags=["de300"]
)

train_task = PythonOperator(
    task_id='temperature_regression',
    python_callable=temperature_regression,
    provide_context=True,
    dag=dag
)