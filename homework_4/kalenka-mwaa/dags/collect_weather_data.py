# -*- coding: utf-8 -*-
"""collect_weather_data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UlvbKh94xUF9lYhywfWuWCqwlIu_AkEX
"""

from airflow import DAG
from airflow.operators.python import PythonOperator, BranchPythonOperator
from airflow.operators.empty import EmptyOperator

import pendulum
import requests
import pandas as pd

import boto3
import io

import time
from datetime import datetime, timedelta
import pytz
central = pytz.timezone("US/Central")

S3_BUCKET = "kalenka-mwaa"
S3_PREFIX = "weather_data/"

WORKFLOW_SCHEDULE = "0 */2 * * *"
WEATHER_STATIONS = ["KORD", "KENW", "KMDW", "KPNT"]


# Task Definition
def collect_weather_data(**kwargs):
    BASE_URL = "https://api.weather.gov/stations/{station}/observations/latest"
    collected_data = {}

    for station in WEATHER_STATIONS:
        url = BASE_URL.format(station=station)
        response = requests.get(url)
        if response.status_code == 200:
            collected_data[station] = response.json()
        else:
            collected_data[station] = {"error": "Failed to fetch"}
        time.sleep(1)

    time_of_collection = datetime.utcnow().replace(second=0, microsecond=0).isoformat()
    records = []
    # Get each desired value at each station
    # This does not account for missing/null data. That is done in the prediction
    for station, data in collected_data.items():
        props = data.get("properties", {})
        records.append({
            "timeOfCollection": time_of_collection,
            "timestamp": props.get("timestamp"),
            "station": station,
            "temperature": props.get("temperature", {}).get("value"),
            "dewpoint": props.get("dewpoint", {}).get("value"),
            "windSpeed": props.get("windSpeed", {}).get("value"),
            "barometricPressure": props.get("barometricPressure", {}).get("value"),
            "visibility": props.get("visibility", {}).get("value"),
            "precipitationLastHour": props.get("precipitationLastHour", {}).get("value"),
            "relativeHumidity": props.get("relativeHumidity", {}).get("value"),
            "heatIndex": props.get("heatIndex", {}).get("value"),
        })

    df = pd.DataFrame(records)
    csv_buffer = io.StringIO()
    df.to_csv(csv_buffer, index=False)

    # Timestamp
    ts = datetime.now(central).replace(second=0, microsecond=0).strftime('%Y%m%dT%H%M%S')
    key = f"{S3_PREFIX}weather_data_{ts}.csv"

    s3 = boto3.client("s3")
    s3.put_object(Bucket=S3_BUCKET, Key=key, Body=csv_buffer.getvalue())

# DAG Setup

# Default arguments dictionary for the DAG execution
default_args = {
    'owner': 'kalenka',  # Owner of the DAG
    'depends_on_past': False,  # Ensures tasks do not depend on past runs
    'start_date': datetime(2025, 6, 5),
    'retries': 1,  # Number of retry attempts upon failure
}


dag = DAG(
    'q2',  # Name of the DAG
    default_args=default_args,
    description='Make a csv of weather data every 2hr',
    schedule=WORKFLOW_SCHEDULE,  # Schedule interval for DAG execution
    tags=["de300"]  # DAG tagging for categorization
)

export = PythonOperator(
    task_id="collect_weather_data",
    python_callable=collect_weather_data,
    provide_context=True,
    dag=dag
)